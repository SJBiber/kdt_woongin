"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° URL ìˆ˜ì§‘ ëª¨ë“ˆ
ì •ì  í¬ë¡¤ë§ ìš°ì„ , í•„ìš”ì‹œ ë„¤ì´ë²„ ê²€ìƒ‰ API ì‚¬ìš©
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import quote
import time
from config.settings import NAVER_CLIENT_ID, NAVER_CLIENT_SECRET, MAX_NEWS_COUNT


class NaverNewsSearcher:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° URL ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        self.use_api = bool(NAVER_CLIENT_ID and NAVER_CLIENT_SECRET)
        
        if self.use_api:
            print("âœ… ë„¤ì´ë²„ ê²€ìƒ‰ API ì‚¬ìš©")
        else:
            print("âœ… ì›¹ í¬ë¡¤ë§ ë°©ì‹ ì‚¬ìš© (API ë¯¸ì„¤ì •)")
    
    def search_news_by_api(self, keyword: str, max_count: int = MAX_NEWS_COUNT) -> List[Dict]:
        """
        ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•œ ë‰´ìŠ¤ ê²€ìƒ‰ (í˜ì´ì§• ì§€ì›)
        """
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            'X-Naver-Client-Id': NAVER_CLIENT_ID,
            'X-Naver-Client-Secret': NAVER_CLIENT_SECRET
        }
        
        news_list = []
        display = 100
        # API ìµœëŒ€ í—ˆìš© start ê°’ì€ 1000ì…ë‹ˆë‹¤.
        for start in range(1, min(max_count, 1001), display):
            params = {
                'query': keyword,
                'display': min(display, max_count - len(news_list)),
                'start': start,
                'sort': 'date'
            }
            
            try:
                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                data = response.json()
                
                items = data.get('items', [])
                if not items:
                    break
                    
                for item in items:
                    news_list.append({
                        'title': self._clean_html_tags(item['title']),
                        'link': item['link'],
                        'description': self._clean_html_tags(item['description']),
                        'pubDate': item['pubDate'],
                        'originallink': item.get('originallink', item['link'])
                    })
                
                if len(news_list) >= max_count:
                    break
                    
                time.sleep(0.1) # ì§§ì€ ì§€ì—°
                
            except Exception as e:
                print(f"âŒ API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (start={start}): {e}")
                break
        
        print(f"âœ… APIë¡œ {len(news_list)}ê°œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")
        return news_list
    
    def search_news_by_crawling(self, keyword: str, max_count: int = MAX_NEWS_COUNT, start_date: str = None, end_date: str = None) -> List[Dict]:
        """
        ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ë‰´ìŠ¤ ê²€ìƒ‰ (ë‚ ì§œ í•„í„°ë§ ì§€ì›)
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            start_date: ì‹œì‘ì¼ (YYYY.MM.DD í˜•ì‹)
            end_date: ì¢…ë£Œì¼ (YYYY.MM.DD í˜•ì‹)
        """
        encoded_keyword = quote(keyword)
        news_list = []
        
        # ë‚ ì§œ í•„í„°ë§ íŒŒë¼ë¯¸í„° êµ¬ì„± (í™•ì¸ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ ì›¹ ê·œê²©)
        date_query = ""
        if end_date:
            if not start_date:
                start_date = "2024.12.01" # ìµœê·¼ í•œ ë‹¬ ì •ë„ (ìˆ˜ì •)
            
            # YYYY.MM.DD -> YYYYMMDD ë³€í™˜ (ì  ì œê±° í•„ìˆ˜)
            s_num = start_date.replace(".", "").strip()
            e_num = end_date.replace(".", "").strip()
            
            # ë„¤ì´ë²„ ì›¹ì—ì„œ ì¶”ì¶œí•œ ê°€ì¥ í™•ì‹¤í•œ íŒŒë¼ë¯¸í„° ì¡°í•©
            # pd=3: ê¸°ê°„ ì„¤ì • ëª¨ë“œ
            # ds/de: ê²€ìƒ‰ì°½ í‘œì‹œìš©
            # nso: ê²€ìƒ‰ ì—”ì§„ í•„í„°ë§ìš© (so:ddëŠ” ìµœì‹ ìˆœ, p:from...to... ëŠ” ê¸°ê°„)
            date_query = f"&pd=3&ds={start_date}&de={end_date}&nso=so:dd,p:from{s_num}to{e_num},a:all"
            
        print(f"ğŸ•µï¸ ê¸°ê°„ ê²€ìƒ‰ í™œì„±: {start_date} ~ {end_date}")
        
        for start in range(1, max_count + 1, 10):
            # nso ê¸°ë°˜ URL (sort=1 ê°™ì€ ë‹¤ë¥¸ ì •ë ¬ íŒŒë¼ë¯¸í„°ì™€ ì¶©ëŒ ë°©ì§€)
            url = f"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&start={start}{date_query}"
            
            try:
                response = requests.get(url, headers=self.headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                news_items = soup.select('div.news_area')
                if not news_items:
                    print("âš ï¸ ë” ì´ìƒ ë°œê²¬ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                for item in news_items:
                    if len(news_list) >= max_count:
                        break
                    
                    title_elem = item.select_one('a.news_tit')
                    if not title_elem:
                        continue
                    
                    link = title_elem.get('href', '')
                    title = title_elem.get_text(strip=True)
                    
                    desc_elem = item.select_one('div.news_dsc')
                    description = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    news_list.append({
                        'title': title,
                        'link': link,
                        'description': description,
                        'pubDate': '' 
                    })
                
                if len(news_list) % 100 == 0:
                    print(f"â³ í˜„ì¬ {len(news_list)}ê°œ ìˆ˜ì§‘ ì¤‘...")
                
                if len(news_list) >= max_count:
                    break
                
                # ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹œ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ì§€ì—° ì‹œê°„ ì¡°ì ˆ
                time.sleep(0.3)
                
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ (start={start}): {e}")
                break
        
        print(f"âœ… í¬ë¡¤ë§ìœ¼ë¡œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return news_list
    
    def search_news(self, keyword: str, max_count: int = MAX_NEWS_COUNT, start_date: str = None, end_date: str = None) -> List[Dict]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ (APIì™€ í¬ë¡¤ë§ ê²°ê³¼ ê²°í•©, ë‚ ì§œ í•„í„°ë§ ì§€ì›)
        """
        # ì¢…ë£Œì¼ë§Œ ìˆê³  ì‹œì‘ì¼ì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ì´ ì•ˆë˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì„ì˜ì˜ ì‹œì‘ì¼ ì„¤ì •
        if end_date and not start_date:
            start_date = "2025.01.01" # ì¶©ë¶„íˆ ê³¼ê±° ì‹œì 
            
        print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ (ëª©í‘œ: {max_count}ê°œ)...")
        
        final_list = []
        
        # 1. íŠ¹ì • ê¸°ê°„ ê²€ìƒ‰ì´ ì•„ë‹ˆê³  API ì‚¬ìš© ê°€ëŠ¥í•  ë•Œ API ê²€ìƒ‰ ìš°ì„  ì‹œë„
        if not (start_date or end_date) and self.use_api:
            api_results = self.search_news_by_api(keyword, max_count)
            final_list.extend(api_results)
        
        # 2. ê¸°ê°„ ê²€ìƒ‰ì´ê±°ë‚˜ ëª©í‘œì¹˜ì— ë„ë‹¬í•˜ì§€ ëª»í–ˆë‹¤ë©´ í¬ë¡¤ë§ìœ¼ë¡œ ë³´ì¶©/ê²€ìƒ‰
        if len(final_list) < max_count:
            if final_list:
                print(f"âš ï¸  API ìˆ˜ì§‘ ì™„ë£Œ ({len(final_list)}ê°œ), ë¶€ì¡±í•œ ë¶€ë¶„ì„ í¬ë¡¤ë§ìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")
            
            existing_links = {item['link'] for item in final_list}
            
            # í¬ë¡¤ë§ ìˆ˜í–‰ (ë‚ ì§œ í•„í„°ë§ í¬í•¨)
            crawl_results = self.search_news_by_crawling(keyword, max_count, start_date, end_date)
            
            for item in crawl_results:
                if item['link'] not in existing_links:
                    final_list.append(item)
                    existing_links.add(item['link'])
                
                if len(final_list) >= max_count:
                    break
        
        print(f"âœ… ìµœì¢…ì ìœ¼ë¡œ {len(final_list)}ê°œ ë‰´ìŠ¤ URLì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        return final_list[:max_count]
    
    @staticmethod
    def _clean_html_tags(text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return BeautifulSoup(text, 'html.parser').get_text()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    searcher = NaverNewsSearcher()
    results = searcher.search_news("AI", max_count=5)
    
    for i, news in enumerate(results, 1):
        print(f"\n{i}. {news['title']}")
        print(f"   URL: {news['link']}")
