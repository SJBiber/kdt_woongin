"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘ ëª¨ë“ˆ
ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ API(cbox)ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ìˆ˜ì§‘
"""

import requests
import json
import time
import hashlib
from typing import List, Dict, Optional
from datetime import datetime
from config.settings import MAX_COMMENTS_PER_NEWS


class NaverCommentCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ í¬ë¡¤ëŸ¬ (API ê¸°ë°˜)"""
    
    def __init__(self, headless: bool = True):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://news.naver.com/',
            'Accept': '*/*',
        }
        print("âœ… ëŒ“ê¸€ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (API ë°©ì‹)")
    
    def close(self):
        """ì¢…ë£Œ ì²˜ë¦¬"""
        pass
    
    def extract_news_info(self, url: str) -> Dict[str, str]:
        """URLì—ì„œ oid, aid ë° ì„¹ì…˜ ì¶”ì¶œ"""
        try:
            oid = ""
            aid = ""
            if '/article/' in url:
                parts = url.split('/article/')[-1].split('/')
                if len(parts) >= 2:
                    oid, aid = parts[0], parts[1].split('?')[0]
            elif 'oid=' in url and 'aid=' in url:
                params = url.split('?')[-1].split('&')
                for param in params:
                    if param.startswith('oid='):
                        oid = param.split('=')[1]
                    elif param.startswith('aid='):
                        aid = param.split('=')[1]
            
            return {
                "oid": oid, 
                "aid": aid,
                "news_id": f"{oid}_{aid}" if oid and aid else hashlib.md5(url.encode()).hexdigest()[:16]
            }
        except Exception:
            return {"oid": "", "aid": "", "news_id": hashlib.md5(url.encode()).hexdigest()[:16]}

    def _get_api_response(self, oid: str, aid: str, template_id: str, page: int):
        """ì‹¤ì œ API í˜¸ì¶œ ë° íŒŒì‹±"""
        # ê°€ì¥ ë²”ìš©ì ì¸ cbox API ì—”ë“œí¬ì¸íŠ¸
        api_url = "https://apis.naver.com/commentBox/cbox/web/commentList.json"
        params = {
            'ticket': 'news',
            'templateId': template_id,
            'pool': 'cbox5',
            'lang': 'ko',
            'country': 'KR',
            'objectId': f'news{oid},{aid}', 
            'pageSize': 20,
            'page': page,
            'sort': 'favorite',
            'initialize': 'true' if page == 1 else 'false',
            'useIntermReactions': 'true',
            'listType': 'OBJECT'
        }
        
        try:
            response = requests.get(api_url, params=params, headers=self.headers)
            if response.status_code != 200:
                return None
            
            res_text = response.text
            if "(" in res_text:
                res_text = res_text[res_text.find("(")+1:res_text.rfind(")")]
            return json.loads(res_text)
        except:
            return None

    def crawl_comments(self, news_url: str, max_comments: int = MAX_COMMENTS_PER_NEWS) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘"""
        info = self.extract_news_info(news_url)
        oid, aid, news_id = info["oid"], info["aid"], info["news_id"]
        
        if not oid or not aid:
            return []

        print(f"ğŸ“° ë‰´ìŠ¤ ID: {news_id} (ìˆ˜ì§‘ ì¤‘...)")
        
        all_comments = []
        page = 1
        
        # ì‹œë„í•´ë³¼ templateId ëª©ë¡
        # ë„¤ì´ë²„ëŠ” ì„¹ì…˜ì— ë”°ë¼ í…œí”Œë¦¿ IDê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜, ë³´í†µ view_newsê°€ ë²”ìš©ì ì…ë‹ˆë‹¤.
        template_ids = ["view_news", "view_economy", "view_society", "view_politics"]
        
        # ì ì ˆí•œ templateId ì°¾ê¸°
        current_template = template_ids[0]
        
        try:
            while len(all_comments) < max_comments:
                data = self._get_api_response(oid, aid, current_template, page)
                
                # ì‹¤íŒ¨ ì‹œ templateId ë°”ê¿”ì„œ ì¬ì‹œë„
                if not data or not data.get('success'):
                    success = False
                    for tid in template_ids[1:]:
                        data = self._get_api_response(oid, aid, tid, page)
                        if data and data.get('success'):
                            current_template = tid
                            success = True
                            break
                    if not success:
                        print("âš ï¸  ëŒ“ê¸€ APIë¥¼ í˜¸ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break
                
                result = data.get('result', {})
                comment_list = result.get('commentList', [])
                
                if not comment_list:
                    break
                
                for item in comment_list:
                    if len(all_comments) >= max_comments:
                        break
                    if item.get('status') != 'ON':
                        continue
                        
                    all_comments.append({
                        'comment_id': str(item.get('commentNo')),
                        'news_id': news_id,
                        'author': item.get('userName', 'ë¹„ê³µê°œ'),
                        'content': item.get('contents', ''),
                        'likes': item.get('sympathyCount', 0),
                        'dislikes': item.get('antipathyCount', 0),
                        'published_at': item.get('modTime', item.get('regTime', datetime.now().isoformat())),
                        'sentiment_label': None,
                        'sentiment_score': None,
                        'llm_sentiment': None,
                        'keywords': None
                    })
                
                page_info = result.get('pageModel', {})
                if page >= page_info.get('totalPages', 0):
                    break
                page += 1
                time.sleep(0.1)

            print(f"âœ… {len(all_comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
            return all_comments

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []


if __name__ == "__main__":
    crawler = NaverCommentCrawler()
    test_url = "https://n.news.naver.com/mnews/article/243/0000091781?sid=101"
    comments = crawler.crawl_comments(test_url, max_comments=10)
    for c in comments:
        print(f"[{c['author']}] {c['content'][:30]}...")
