"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘í•˜ì—¬ DBì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
(API ì „ ì „ìš©)
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from src.collector.news_searcher import NaverNewsSearcher
from src.collector.comment_crawler import NaverCommentCrawler
from database.supabase_manager import SupabaseManager
from config.settings import validate_config, MAX_NEWS_COUNT
from datetime import datetime
import time


def collect_news_articles(keyword: str, max_news: int = MAX_NEWS_COUNT, end_date: str = None):
    """
    í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ ê¸°ì‚¬ ì •ë³´ë§Œ DB ì €ì¥ (ë‚ ì§œ í•„í„° ì§€ì›)
    """
    print("=" * 60)
    print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“Œ ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
    print(f"ğŸ“Œ ìˆ˜ì§‘ ê¸°ì¤€: {end_date + ' ì´ì „ ê¸°ì‚¬' if end_date else 'ìµœì‹  ê¸°ì‚¬'}")
    print(f"ğŸ“Œ ìˆ˜ì§‘ ë‰´ìŠ¤ ìˆ˜: ìµœëŒ€ {max_news}ê°œ")
    print("=" * 60)
    
    # 1. í™˜ê²½ ì„¤ì • ê²€ì¦
    try:
        validate_config()
    except ValueError as e:
        print(f"\nâŒ {e}")
        return
    
    # 2. ë‰´ìŠ¤ ê²€ìƒ‰
    searcher = NaverNewsSearcher()
    # íŠ¹ì • ì‹œì  ì´ì „ ìˆ˜ì§‘ì„ ìœ„í•´ end_date ì „ë‹¬
    news_list = searcher.search_news(keyword, max_news, end_date=end_date)
    
    if not news_list:
        print("\nâŒ ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
    
    # 3. DB ì €ì¥
    db = SupabaseManager()
    crawler_temp = NaverCommentCrawler()
    
    unique_articles = {} # news_idë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    
    for news in news_list:
        news_info = crawler_temp.extract_news_info(news['link'])
        news_id = news_info['news_id']
        
        # ë‚ ì§œ ì²˜ë¦¬
        pub_date = news.get('pubDate')
        if not pub_date:
            pub_date = datetime.now().isoformat()
            
        article_data = {
            'news_id': news_id,
            'title': news['title'],
            'link': news['link'],
            'description': news.get('description', ''),
            'pub_date': pub_date,
            'origin_link': news.get('originallink', news['link'])
        }
        
        if news_id not in unique_articles:
            unique_articles[news_id] = article_data
    
    articles_to_save = list(unique_articles.values())
    
    if articles_to_save:
        print(f"ğŸ’¾ ê¸°ì‚¬ {len(articles_to_save)}ê°œ ì €ì¥ ì¤‘ (ì¤‘ë³µ ì œì™¸)...")
        saved_count = db.insert_articles_batch(articles_to_save)
        print(f"âœ… {saved_count}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
    
    print("\n" + "=" * 60)
    print("âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    from datetime import datetime
    print("\nğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ê¸° (News Only)")
    print("-" * 60)
    keyword = input("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    print("\nğŸ“… íŠ¹ì • ë‚ ì§œ ì´ì „ì˜ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("   í˜•ì‹: YYYY.MM.DD (ì˜ˆ: 2026.01.19)")
    print("   ì—”í„°ë¥¼ ì¹˜ë©´ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    target_date = input("ì…ë ¥: ").strip()
    
    if keyword:
        collect_news_articles(keyword, end_date=target_date if target_date else None)
    else:
        print("âŒ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
