import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from supabase import create_client
import os
from dotenv import load_dotenv
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„ì„±ê·¼ ì „ì²´ ìš”ì•½", page_icon="1ï¸âƒ£", layout="wide")

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@st.cache_resource
def get_supabase_client():
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")
    return create_client(url, key)

@st.cache_data(ttl=300)
def load_im_data():
    """ì„ì„±ê·¼ ë°ì´í„° ë¡œë“œ"""
    client = get_supabase_client()
    all_data = []
    page_size = 1000
    offset = 0
    
    while True:
        response = client.table("im_sung_gen_youtube_comments")\
            .select("*")\
            .range(offset, offset + page_size - 1)\
            .execute()
        
        batch_data = response.data
        if not batch_data:
            break
            
        all_data.extend(batch_data)
        if len(batch_data) < page_size:
            break
        offset += page_size
        
    df = pd.DataFrame(all_data)
    
    if not df.empty:
        df['published_at'] = pd.to_datetime(df['published_at'])
        df = df.sort_values('published_at', ascending=False)
        df = df.drop_duplicates(subset=['comment_id'], keep='first')
        
        # ê°ì • ê·¸ë£¹í™” (0,2:ê¸ì •, 1,3,4:ë¶€ì •, 5:ê·¸ì™¸)
        def group_sentiment(val):
            if val in [0, 2]: return "ê¸ì •"
            if val in [1, 3, 4]: return "ë¶€ì •"
            if val == 5: return "ê·¸ì™¸"
            return "ë¯¸ë¶„ë¥˜"

        df['sentiment_group'] = df['llm_sentiment'].apply(group_sentiment)
        
        # ìƒì„¸ ë¼ë²¨
        label_map = {0: "ì§€ì§€", 1: "ë¶„ë…¸", 2: "ì¤‘ë¦½", 3: "ì‹¤ë§", 4: "ì¡°ë¡±", 5: "ê·¸ì™¸"}
        df['llm_label'] = df['llm_sentiment'].map(label_map)
        
        # ì‹œê¸° êµ¬ë¶„ (1ì›” 19ì¼ ê¸°ì¤€)
        controversy_date = pd.Timestamp("2026-01-19").tz_localize('UTC')
        if df['published_at'].dt.tz is None:
            df['published_at'] = df['published_at'].dt.tz_localize('UTC')
            
        df['period'] = df['published_at'].apply(lambda x: "ë…¼ë€ í›„" if x >= controversy_date else "ë…¼ë€ ì „")
        
    return df

# ë°ì´í„° ë¡œë“œ
df = load_im_data()

if df.empty:
    st.error("ì„ì„±ê·¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìˆ˜ì§‘ ë° ë¶„ì„ì„ ì§„í–‰í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- í—¤ë” ---
st.title("1ï¸âƒ£ ì„ì„±ê·¼ ì—¬ë¡  ë¶„ì„ - ì „ì²´ ìš”ì•½")
st.markdown(f"**ì´ ëŒ“ê¸€ ìˆ˜:** `{len(df):,}` | **ë¶„ì„ ì™„ë£Œ:** `{df['llm_sentiment'].notnull().sum():,}`")

st.markdown("---")

# --- í•µì‹¬ ë©”íŠ¸ë¦­ ---
st.subheader("ğŸ“Š í•µì‹¬ ì§€í‘œ")
col1, col2, col3, col4 = st.columns(4)

with col1:
    pos_pct = (df['sentiment_group'] == "ê¸ì •").mean() * 100
    st.metric("ì „ì²´ ê¸ì • ë¹„ìœ¨", f"{pos_pct:.1f}%", delta=f"{pos_pct-20:.1f}%p")

with col2:
    neg_pct = (df['sentiment_group'] == "ë¶€ì •").mean() * 100
    delta_color = "inverse" if neg_pct > 50 else "normal"
    st.metric("ì „ì²´ ë¶€ì • ë¹„ìœ¨", f"{neg_pct:.1f}%", delta="High Risk" if neg_pct > 50 else "Normal", delta_color=delta_color)

with col3:
    after_neg = (df[df['period']=='ë…¼ë€ í›„']['sentiment_group']=='ë¶€ì •').mean() * 100
    st.metric("ë…¼ë€ í›„ ë¶€ì • ë¹„ìœ¨", f"{after_neg:.1f}%")

with col4:
    severity_score = after_neg / 10 # ì „ì²´ ë¶€ì • ë¹„ìœ¨ë³´ë‹¤ ë…¼ë€ í›„ ë¶€ì • ë¹„ìœ¨ë¡œ ì‹¬ê°ë„ ê³„ì‚°
    st.metric("ì‹¬ê°ë„ ì ìˆ˜", f"{severity_score:.1f}/10", delta="ë§¤ìš° ì‹¬ê°" if severity_score > 7 else "ë³´í†µ")

st.markdown("---")

# --- ê°ì„± ë¶„í¬ ë° ì‹œê¸°ë³„ ë¹„êµ ---
col_left, col_right = st.columns(2)

with col_left:
    st.subheader("ğŸ“Š ì „ì²´ ê°ì„± ê·¸ë£¹ ë¶„í¬")
    group_counts = df['sentiment_group'].value_counts().reset_index()
    group_counts.columns = ['Group', 'Count']
    fig_pie = px.pie(
        group_counts, 
        values='Count', 
        names='Group',
        color='Group', 
        color_discrete_map={"ê¸ì •": "#00CC96", "ë¶€ì •": "#EF553B", "ê·¸ì™¸": "#AB63FA"},
        hole=0.4,
        labels={'Group': 'ê°ì • ê·¸ë£¹', 'Count': 'ëŒ“ê¸€ ìˆ˜'}
    )
    fig_pie.update_traces(textposition='inside', textinfo='percent+label')
    st.plotly_chart(fig_pie, width="stretch")

with col_right:
    st.subheader("ğŸ“… ë…¼ë€ ì „/í›„ ì—¬ë¡  ë¹„êµ (1ì›” 19ì¼ ê¸°ì¤€)")
    period_df = df.groupby(['period', 'sentiment_group']).size().reset_index(name='count')
    period_totals = period_df.groupby('period')['count'].transform('sum')
    period_df['percentage'] = (period_df['count'] / period_totals) * 100
    
    fig_comp = px.bar(
        period_df, 
        x='period', 
        y='percentage', 
        color='sentiment_group',
        barmode='group', 
        text_auto='.1f',
        color_discrete_map={"ê¸ì •": "#00CC96", "ë¶€ì •": "#EF553B", "ê·¸ì™¸": "#AB63FA"},
        category_orders={"period": ["ë…¼ë€ ì „", "ë…¼ë€ í›„"]},
        labels={'period': 'ì‹œê¸°', 'percentage': 'ë¹„ìœ¨ (%)', 'sentiment_group': 'ê°ì • ê·¸ë£¹'}
    )
    fig_comp.update_traces(textposition='outside', cliponaxis=False)
    fig_comp.update_layout(yaxis_title="ë¹„ìœ¨ (%)", height=400, yaxis_range=[0, period_df['percentage'].max() * 1.15])
    st.plotly_chart(fig_comp, width="stretch")

st.markdown("---")

# --- 6ê°€ì§€ ê°ì • ìƒì„¸ ë¶„í¬ (ë…¼ë€ ì „ vs í›„ ë¹„êµ) ---
st.subheader("ğŸ­ ë…¼ë€ ì „/í›„ 6ê°€ì§€ ê°ì • ë³€í™” ë¹„êµ")

# ì‹œê¸°ë³„/ê°ì •ë³„ ì§‘ê³„
sentiment_comp = df.groupby(['period', 'llm_label']).size().reset_index(name='count')

# ì‹œê¸°ë³„ ì´í•© ê³„ì‚° (ë¹„ìœ¨ ì‚°ì¶œìš©)
period_sums = sentiment_comp.groupby('period')['count'].transform('sum')
sentiment_comp['percentage'] = (sentiment_comp['count'] / period_sums * 100).round(1)

# ê°ì •ë³„ ìƒ‰ìƒ ë§µ (ë…¼ë¦¬ì  ìƒ‰ìƒ ë°°ì¹˜)
emotion_color_map = {
    "ì§€ì§€": "#00CC96",      # ì´ˆë¡ (ê¸ì •)
    "ë¶„ë…¸": "#EF553B",      # ë¹¨ê°• (ê°•í•œ ë¶€ì •)
    "ì¤‘ë¦½": "#636EFA",      # íŒŒë‘ (ì¤‘ë¦½)
    "ì‹¤ë§": "#FFA15A",      # ì£¼í™© (ë¶€ì •)
    "ì¡°ë¡±": "#AB63FA",      # ë³´ë¼ (ë¶€ì •)
    "ê·¸ì™¸": "#B6E880"       # ì—°ë‘ (ê¸°íƒ€)
}

# Grouped Bar Chart
fig_bar = px.bar(
    sentiment_comp,
    x='llm_label',
    y='percentage',
    color='period',
    barmode='group',
    text='percentage',
    labels={'llm_label': 'ê°ì •', 'percentage': 'ë¹„ìœ¨ (%)', 'period': 'ì‹œê¸°'},
    category_orders={"period": ["ë…¼ë€ ì „", "ë…¼ë€ í›„"]}  # ìˆœì„œ ê³ ì •
)

fig_bar.update_traces(texttemplate='%{text:.1f}%', textposition='outside', cliponaxis=False)
fig_bar.update_layout(height=450, yaxis_title="ë¹„ìœ¨ (%)", yaxis_range=[0, sentiment_comp['percentage'].max() * 1.15])
st.plotly_chart(fig_bar, width="stretch")

st.markdown("---")

# --- ì›Œë“œí´ë¼ìš°ë“œ ---
st.subheader("â˜ï¸ ê°ì • ê·¸ë£¹ë³„ í•µì‹¬ í‚¤ì›Œë“œ")
wc_target = st.selectbox("ì›Œë“œí´ë¼ìš°ë“œ ëŒ€ìƒ ê·¸ë£¹ ì„ íƒ", ["ì „ì²´", "ê¸ì •", "ë¶€ì •", "ê·¸ì™¸"])

if wc_target == "ì „ì²´":
    wc_df = df
else:
    wc_df = df[df['sentiment_group'] == wc_target]

all_kws = []
for k in wc_df['keywords'].dropna():
    all_kws.extend(k)

filtered_kws = [word for word in all_kws if len(word) > 1]

if filtered_kws:
    font_paths = [
        '/System/Library/Fonts/Supplemental/AppleGothic.ttf',
        '/Library/Fonts/NanumGothic.ttf',
        'C:/Windows/Fonts/malgun.ttf'
    ]
    selected_font = next((fp for fp in font_paths if os.path.exists(fp)), None)
    
    wc_color = "Greens" if wc_target == "ê¸ì •" else "Reds" if wc_target == "ë¶€ì •" else "Purples"
    wc = WordCloud(
        font_path=selected_font,
        width=1200,
        height=400,
        background_color='white',
        colormap=wc_color
    ).generate(" ".join(filtered_kws))
    
    st.image(wc.to_image(), width="stretch")
    
    # í‚¤ì›Œë“œ Top 10
    st.caption(f"ğŸ“Œ {wc_target} ëŒ“ê¸€ì˜ ì£¼ìš” í‚¤ì›Œë“œ TOP 10")
    top_10 = Counter(filtered_kws).most_common(10)
    t10_df = pd.DataFrame(top_10, columns=['ë‹¨ì–´', 'ë¹ˆë„'])
    fig_t10 = px.bar(
        t10_df,
        x='ë¹ˆë„',
        y='ë‹¨ì–´',
        orientation='h',
        color='ë¹ˆë„',
        color_continuous_scale=wc_color
    )
    fig_t10.update_layout(yaxis={'categoryorder':'total ascending'}, height=300)
    st.plotly_chart(fig_t10, width="stretch")
else:
    st.info("í•´ë‹¹ ê·¸ë£¹ì— ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("---")

# --- ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ---
st.subheader("ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")

# ê³„ì‚°
before_df = df[df['period'] == 'ë…¼ë€ ì „']
after_df = df[df['period'] == 'ë…¼ë€ í›„']

before_neg = (before_df['sentiment_group'] == 'ë¶€ì •').mean() * 100 if not before_df.empty else 0
after_neg = (after_df['sentiment_group'] == 'ë¶€ì •').mean() * 100 if not after_df.empty else 0
neg_increase = after_neg - before_neg

# ê°€ì¥ ë§ì€ ê°ì •
top_emotion_after = after_df['llm_label'].value_counts().index[0] if not after_df.empty else "N/A"
top_emotion_pct = (after_df['llm_label'].value_counts().iloc[0] / len(after_df) * 100) if not after_df.empty else 0

# í‰ê·  ì¢‹ì•„ìš” (ê°ì •ë³„)
if 'likes' in df.columns:
    avg_likes_by_sentiment = df.groupby('llm_label')['likes'].mean().sort_values(ascending=False)
    most_liked_emotion = avg_likes_by_sentiment.index[0]
    most_liked_avg = avg_likes_by_sentiment.iloc[0]
else:
    most_liked_emotion = "N/A"
    most_liked_avg = 0

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown(f"""
    ### ğŸ“Š ì—¬ë¡  ë³€í™”

    - **ë…¼ë€ ì „ ë¶€ì • ë¹„ìœ¨**: {before_neg:.1f}%
    - **ë…¼ë€ í›„ ë¶€ì • ë¹„ìœ¨**: {after_neg:.1f}%
    - **ì¦ê°€í­**: **{neg_increase:+.1f}%p**

    {'ğŸ”´ **ì‹¬ê°í•œ ì•…í™”**' if neg_increase > 30 else 'ğŸŸ¡ **ì¤‘ê°„ ì •ë„ ì•…í™”**' if neg_increase > 15 else 'ğŸŸ¢ **ê²½ë¯¸í•œ ë³€í™”**'}

    ë…¼ë€ ì´í›„ ë¶€ì • ì—¬ë¡ ì´ ê¸‰ê²©íˆ ì¦ê°€í–ˆìœ¼ë©°,
    {'í˜„ì¬ ìœ„ê¸° ìƒí™©ì…ë‹ˆë‹¤.' if after_neg > 70 else 'ë¶€ì •ì  ì—¬ë¡ ì´ ìš°ì„¸í•©ë‹ˆë‹¤.' if after_neg > 50 else 'ì—¬ë¡ ì´ ì–‘ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.'}
    """)

with col2:
    st.markdown(f"""
    ### ğŸ­ ê°ì • ë¶„ì„

    - **ì£¼ìš” ê°ì • (ë…¼ë€ í›„)**: **{top_emotion_after}** ({top_emotion_pct:.1f}%)
    - **ì „ì²´ ë¶€ì • ë¹„ìœ¨**: {neg_pct:.1f}%
    - **ì „ì²´ ê¸ì • ë¹„ìœ¨**: {pos_pct:.1f}%

    {
    'ğŸ˜¡ ë¶„ë…¸(ë¶„ë…¸)ê°€ ì§€ë°°ì ì´ë©° ê°•í•œ ë¹„ë‚œ ì—¬ë¡ ' if top_emotion_after == 'ë¶„ë…¸'
    else 'ğŸ˜” ì‹¤ë§(ì‹¤ë§)ì´ ì£¼ë¥¼ ì´ë£¨ë©° ì‹ ë¢° í•˜ë½' if top_emotion_after == 'ì‹¤ë§'
    else 'ğŸ˜ ë¹„ê¼¬ëŠ”(ì¡°ë¡±) ëŒ“ê¸€ì´ ë§ì•„ ì¡°ë¡± ë¶„ìœ„ê¸°' if top_emotion_after == 'ì¡°ë¡±'
    else 'ğŸ‘ ì§€ì§€(ì§€ì§€) ì—¬ë¡ ì´ ìš°ì„¸' if top_emotion_after == 'ì§€ì§€'
    else 'ì¤‘ë¦½(ì¤‘ë¦½) ì˜ê²¬ì´ ë‹¤ìˆ˜'
    }
    """)

with col3:
    st.markdown(f"""
    ### â­ ëŒ€ì¤‘ ê³µê°ë„

    - **ê°€ì¥ ê³µê°ë°›ëŠ” ê°ì •**: **{most_liked_emotion}**
    - **í‰ê·  ì¢‹ì•„ìš”**: {most_liked_avg:.1f}ê°œ

    {
    'ë¶„ë…¸ ëŒ“ê¸€ì´ ë†’ì€ ê³µê°ì„ ë°›ê³  ìˆì–´ ëŒ€ì¤‘ì˜ ê°•í•œ ë¶€ì • ê°ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.' if most_liked_emotion == 'ë¶„ë…¸'
    else 'ì‹¤ë§ ëŒ“ê¸€ì´ ê³µê°ì„ ë°›ê³  ìˆì–´ ì‹ ë¢°ê°€ í¬ê²Œ í•˜ë½í–ˆìŠµë‹ˆë‹¤.' if most_liked_emotion == 'ì‹¤ë§'
    else 'ë¹„ê¼¬ëŠ” ëŒ“ê¸€ì´ ê³µê°ì„ ë°›ê³  ìˆì–´ ì¡°ë¡±ì´ ì£¼ë¥˜ì…ë‹ˆë‹¤.' if most_liked_emotion == 'ì¡°ë¡±'
    else 'ì§€ì§€ ëŒ“ê¸€ì´ ê³µê°ì„ ë°›ê³  ìˆì–´ ê¸ì •ì  ì§€ì§€ì¸µì´ ìˆìŠµë‹ˆë‹¤.' if most_liked_emotion == 'ì§€ì§€'
    else 'ì¤‘ë¦½ì  ì˜ê²¬ì´ ê³µê°ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤.'
    }
    """)

st.markdown("---")

# --- ìƒì„¸ ë°ì´í„° ---
st.subheader("ğŸ’¬ ë¶„ì„ ìƒì„¸ ë°ì´í„°")
with st.expander("ë°ì´í„° ë³´ê¸°/ìˆ¨ê¸°ê¸°"):
    display_df = df[['published_at', 'author', 'content', 'sentiment_group', 'llm_label', 'likes']].sort_values('published_at', ascending=False)
    st.dataframe(display_df, width="stretch")

st.caption("Opinion Analysis Project | Powered by DeepSeek LLM")
