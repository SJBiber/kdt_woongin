import logging
from database.supabase_client import SupabaseManager
from analyzer.nlp_engine import NLPEngine
from tqdm import tqdm
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_normalization():
    db = SupabaseManager()
    # ë§ì¶¤ë²• êµì • ë¹„í™œì„±í™” (ì†ë„ ìµœì í™”: 3-5ë°° í–¥ìƒ)
    nlp = NLPEngine(use_corrector=False)
    
    logger.info("=== [Stage 2] í…ìŠ¤íŠ¸ ì •ê·œí™” ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ ===")
    logger.info("âš¡ ë§ì¶¤ë²• êµì • ë¹„í™œì„±í™” - ê³ ì† ì²˜ë¦¬ ëª¨ë“œ")
    
    total_processed = 0
    total_time = 0
    
    while True:
        try:
            # í…ìŠ¤íŠ¸ ì •ì œ(keywords)ê°€ ì™„ë£Œë˜ì§€ ì•Šì€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (video_id í¬í•¨)
            response = db.client.table("im_sung_gen_youtube_comments")\
                .select("comment_id, content, video_id")\
                .is_("keywords", "null")\
                .limit(50)\
                .execute()
            
            rows = response.data
            if not rows:
                logger.info("âœ… ëª¨ë“  ë°ì´í„°ì˜ ì •ê·œí™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if total_processed > 0:
                    avg_time = total_time / total_processed
                    logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬: {total_processed}ê°œ, í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ/ë°°ì¹˜")
                break

            batch_start = time.time()
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë§ì¶¤ë²• êµì • ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì„±ëŠ¥ ìµœì í™”)
            contents = [row["content"] for row in rows]
            clean_texts = nlp.preprocess_batch(contents)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œë³„ ì²˜ë¦¬ í•„ìš”)
            updated_data = []
            for row, clean_text in tqdm(
                zip(rows, clean_texts), 
                total=len(rows),
                desc="Extracting keywords"
            ):
                keywords = nlp.extract_keywords(clean_text)
                
                updated_data.append({
                    "comment_id": row["comment_id"],
                    "video_id": row["video_id"],
                    "content": row["content"], # ë°ì´í„° ë¬´ê²°ì„±ì„ ìœ„í•´ content í¬í•¨
                    "keywords": keywords
                })

            if updated_data:
                db.upsert_comments(updated_data)
                
                batch_time = time.time() - batch_start
                total_time += batch_time
                total_processed += 1
                
                logger.info(f"âœ… {len(updated_data)}ê°œ ëŒ“ê¸€ ì •ê·œí™” ì™„ë£Œ (ì†Œìš” ì‹œê°„: {batch_time:.2f}ì´ˆ)")
                
        except Exception as e:
            logger.error(f"Error during normalization: {e}")
            time.sleep(5) # ì—ëŸ¬ ì‹œ ì ì‹œ ëŒ€ê¸°

if __name__ == "__main__":
    run_normalization()

