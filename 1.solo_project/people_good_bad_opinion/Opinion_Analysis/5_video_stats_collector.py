"""
YouTube ì˜ìƒ í†µê³„ ìˆ˜ì§‘ê¸°
- ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”ìˆ˜ë¥¼ ìˆ˜ì§‘í•˜ì—¬ 'ì¢‹ì•„ìš” ë¹„ìœ¨(Engagement Rate)'ì„ ê³„ì‚°
- youtube_link.txtì—ì„œ ì˜ìƒ ë§í¬ë¥¼ ì½ì–´ ì²˜ë¦¬
"""

import logging
import re
import json
import csv
from datetime import datetime
from pathlib import Path
from database.supabase_client import SupabaseManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ë…¼ë€ ê¸°ì¤€ì¼
CONTROVERSY_DATE = "2026-01-19"

def extract_video_id(url):
    """YouTube URLì—ì„œ video_id ì¶”ì¶œ"""
    url = url.strip()

    # shorts í˜•ì‹
    if '/shorts/' in url:
        match = re.search(r'/shorts/([a-zA-Z0-9_-]+)', url)
        if match:
            return match.group(1)

    # ì¼ë°˜ watch í˜•ì‹
    if 'v=' in url:
        match = re.search(r'v=([a-zA-Z0-9_-]+)', url)
        if match:
            return match.group(1)

    # youtu.be í˜•ì‹
    if 'youtu.be/' in url:
        match = re.search(r'youtu\.be/([a-zA-Z0-9_-]+)', url)
        if match:
            return match.group(1)

    return None

def load_video_links(file_path="youtube_link.txt"):
    """youtube_link.txtì—ì„œ ì˜ìƒ ë§í¬ì™€ ë©”íƒ€ì •ë³´(ë‚ ì§œ, ì„¤ëª…) ë¡œë“œ"""
    links = []
    current_comment = ""

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()

            # ë¹ˆ ì¤„ ìŠ¤í‚µ
            if not line:
                continue

            # ì£¼ì„ ì²˜ë¦¬ (ì„¤ëª… ì €ì¥)
            if line.startswith('#'):
                # ì£¼ì„ì²˜ë¦¬ëœ ë§í¬ëŠ” ìŠ¤í‚µ
                if 'http' in line or 'youtube' in line.lower():
                    continue
                current_comment = line[1:].strip()
                continue

            # URLì¸ ê²½ìš°
            if 'youtube.com' in line or 'youtu.be' in line:
                video_id = extract_video_id(line)
                if video_id:
                    # ë‚ ì§œ ì¶”ì¶œ ì‹œë„ (ì˜ˆ: "2026 1ì›” 15ì¼", "2025 12ì›” 29ì¼")
                    date_match = re.search(r'(\d{4})\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', current_comment)
                    upload_date = None
                    if date_match:
                        year, month, day = date_match.groups()
                        upload_date = f"{year}-{int(month):02d}-{int(day):02d}"

                    links.append({
                        'url': line,
                        'video_id': video_id,
                        'description': current_comment,
                        'upload_date': upload_date
                    })
                current_comment = ""

    return links

def fetch_video_stats_yt_dlp(url):
    """yt-dlpë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒ í†µê³„ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import yt_dlp

        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'skip_download': True,
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)

            return {
                'title': info.get('title', 'Unknown'),
                'view_count': info.get('view_count', 0),
                'like_count': info.get('like_count', 0),
                'comment_count': info.get('comment_count', 0),
                'duration': info.get('duration', 0),
                'upload_date': info.get('upload_date', ''),  # YYYYMMDD í˜•ì‹
                'channel': info.get('channel', 'Unknown'),
            }
    except ImportError:
        logger.error("yt-dlpê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install yt-dlp'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        logger.error(f"ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url} - {e}")
        return None

def calculate_engagement_rate(view_count, like_count):
    """ì¢‹ì•„ìš” ë¹„ìœ¨(Engagement Rate) ê³„ì‚°"""
    if view_count == 0:
        return 0
    return (like_count / view_count) * 100

def collect_all_video_stats():
    """ëª¨ë“  ì˜ìƒì˜ í†µê³„ ìˆ˜ì§‘"""
    links = load_video_links()
    logger.info(f"ì´ {len(links)}ê°œì˜ ì˜ìƒ ë§í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    results = []

    for i, link_info in enumerate(links, 1):
        url = link_info['url']
        logger.info(f"[{i}/{len(links)}] ìˆ˜ì§‘ ì¤‘: {link_info['description'][:30]}...")

        stats = fetch_video_stats_yt_dlp(url)

        if stats:
            # ì—…ë¡œë“œ ë‚ ì§œ ì •ë¦¬ (yt-dlpëŠ” YYYYMMDD í˜•ì‹)
            upload_date = link_info.get('upload_date')
            if not upload_date and stats.get('upload_date'):
                raw_date = stats['upload_date']
                if len(raw_date) == 8:
                    upload_date = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:8]}"

            # ë…¼ë€ ì „/í›„ íŒë‹¨
            is_before_controversy = None
            if upload_date:
                is_before_controversy = upload_date < CONTROVERSY_DATE

            engagement_rate = calculate_engagement_rate(
                stats['view_count'],
                stats['like_count']
            )

            result = {
                'video_id': link_info['video_id'],
                'url': url,
                'title': stats['title'],
                'description': link_info['description'],
                'channel': stats['channel'],
                'upload_date': upload_date,
                'is_before_controversy': is_before_controversy,
                'view_count': stats['view_count'],
                'like_count': stats['like_count'],
                'comment_count': stats['comment_count'],
                'duration_sec': stats['duration'],
                'engagement_rate': round(engagement_rate, 4),  # ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜ * 100
            }
            results.append(result)

            logger.info(f"  â†’ ì¡°íšŒìˆ˜: {stats['view_count']:,} | ì¢‹ì•„ìš”: {stats['like_count']:,} | ë¹„ìœ¨: {engagement_rate:.2f}%")
        else:
            logger.warning(f"  â†’ ìˆ˜ì§‘ ì‹¤íŒ¨: {url}")

    return results

def save_results(results, json_path="video_stats_results.json", csv_path="video_stats_results.csv"):
    """ê²°ê³¼ë¥¼ JSON ë° CSV íŒŒì¼ë¡œ ì €ì¥"""
    # JSON ì €ì¥
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"ê²°ê³¼ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {json_path}")

    # CSV ì €ì¥
    if results:
        fieldnames = results[0].keys()
        with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        logger.info(f"ê²°ê³¼ê°€ CSV íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_path}")

def print_analysis_summary(results):
    """ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
    if not results:
        logger.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\n" + "="*70)
    print("ğŸ“Š ì˜ìƒ í†µê³„ ë¶„ì„ ìš”ì•½")
    print("="*70)

    # ë…¼ë€ ì „/í›„ ë¶„ë¦¬
    before = [r for r in results if r.get('is_before_controversy') == True]
    after = [r for r in results if r.get('is_before_controversy') == False]
    unknown = [r for r in results if r.get('is_before_controversy') is None]

    print(f"\nğŸ“… ë…¼ë€ ê¸°ì¤€ì¼: {CONTROVERSY_DATE}")
    print(f"   - ë…¼ë€ ì „ ì˜ìƒ: {len(before)}ê°œ")
    print(f"   - ë…¼ë€ í›„ ì˜ìƒ: {len(after)}ê°œ")
    print(f"   - ë‚ ì§œ ë¯¸ìƒ: {len(unknown)}ê°œ")

    def calc_stats(data, label):
        if not data:
            return
        total_views = sum(r['view_count'] for r in data)
        total_likes = sum(r['like_count'] for r in data)
        avg_engagement = sum(r['engagement_rate'] for r in data) / len(data)
        overall_engagement = (total_likes / total_views * 100) if total_views > 0 else 0

        print(f"\n{'ğŸŸ¢' if 'ì „' in label else 'ğŸ”´'} {label}")
        print(f"   ì´ ì¡°íšŒìˆ˜: {total_views:,}")
        print(f"   ì´ ì¢‹ì•„ìš”: {total_likes:,}")
        print(f"   ì „ì²´ ì¢‹ì•„ìš” ë¹„ìœ¨: {overall_engagement:.2f}%")
        print(f"   ì˜ìƒë³„ í‰ê·  ë¹„ìœ¨: {avg_engagement:.2f}%")

    calc_stats(before, "ë…¼ë€ ì „ (í˜¸ê° ì—¬ë¡ )")
    calc_stats(after, "ë…¼ë€ í›„ (ë¹„ë‚œ ì—¬ë¡ )")

    # ê°œë³„ ì˜ìƒ ëª©ë¡
    print("\n" + "-"*70)
    print("ğŸ“‹ ê°œë³„ ì˜ìƒ ìƒì„¸")
    print("-"*70)

    for r in sorted(results, key=lambda x: x.get('upload_date') or ''):
        status = "ğŸŸ¢" if r.get('is_before_controversy') else "ğŸ”´" if r.get('is_before_controversy') == False else "âšª"
        print(f"\n{status} [{r.get('upload_date', 'ë‚ ì§œë¯¸ìƒ')}] {r['title'][:40]}...")
        print(f"   ì±„ë„: {r['channel']}")
        print(f"   ì¡°íšŒìˆ˜: {r['view_count']:,} | ì¢‹ì•„ìš”: {r['like_count']:,}")
        print(f"   ğŸ“ˆ Engagement Rate: {r['engagement_rate']:.2f}%")

def main():
    logger.info("=== YouTube ì˜ìƒ í†µê³„ ìˆ˜ì§‘ ì‹œì‘ ===")

    results = collect_all_video_stats()

    if results:
        save_results(results)
        
        # DB ì €ì¥ ì‹œë„
        db = SupabaseManager()
        if db.client:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤(Supabase)ì— ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            db.upsert_video_stats(results)
        else:
            logger.warning("Supabase ì„¤ì •ì´ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            
        print_analysis_summary(results)
    else:
        logger.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
