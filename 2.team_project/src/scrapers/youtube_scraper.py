import os
import re
from datetime import datetime
from googleapiclient.discovery import build
from dotenv import load_dotenv
from supabase import create_client, Client

# .env ë¡œë“œ
load_dotenv(os.path.join(os.path.dirname(__file__), '../../.env'))

class YouTubeScraper:
    def __init__(self):
        self.api_key = os.getenv("YOUTUBE_API_KEY")
        if not self.api_key:
            raise ValueError("YOUTUBE_API_KEY not found in .env file")
        
        self.youtube = build("youtube", "v3", developerKey=self.api_key)
        
        # Supabase ì´ˆê¸°í™”
        self.supabase_url = os.getenv("SUPABASE_URL")
        self.supabase_key = os.getenv("SUPABASE_KEY")
        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)

    def search_videos(self, query, published_after=None, published_before=None, max_results=10):
        """íŠ¹ì • ê¸°ê°„ì˜ ë¹„ë””ì˜¤ ê²€ìƒ‰"""
        request = self.youtube.search().list(
            q=query,
            part="snippet",
            type="video",
            publishedAfter=published_after,
            publishedBefore=published_before,
            maxResults=max_results,
            order="relevance" # ë˜ëŠ” viewCount
        )
        response = request.execute()
        
        videos = []
        for item in response.get("items", []):
            videos.append({
                "video_id": item["id"]["videoId"],
                "title": item["snippet"]["title"],
                "published_at": item["snippet"]["publishedAt"],
                "channel_title": item["snippet"]["channelTitle"]
            })
        return videos

    def get_video_comments(self, video_id, max_results=100):
        """ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ ìˆ˜ì§‘"""
        comments = []
        try:
            request = self.youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=max_results,
                textFormat="plainText"
            )
            response = request.execute()

            for item in response.get("items", []):
                snippet = item["snippet"]["topLevelComment"]["snippet"]
                comments.append({
                    "comment_id": item["id"],
                    "video_id": video_id,
                    "author": snippet["authorDisplayName"],
                    "text": snippet["textDisplay"],
                    "published_at": snippet["publishedAt"],
                    "like_count": snippet["likeCount"]
                })
        except Exception as e:
            print(f"âš ï¸ Error fetching comments for {video_id}: {e}")
            
        return comments

    def save_comments_to_supabase(self, comments):
        """ìˆ˜ì§‘ëœ ëŒ“ê¸€ì„ Supabaseì— ì €ìž¥ (ë˜ëŠ” ë¶„ì„ìš© ë°ì´í„°ì…‹ êµ¬ì„±)"""
        if not comments:
            return
        
        # í…Œì´ë¸” ì´ë¦„ì€ í”„ë¡œì íŠ¸ ìƒí™©ì— ë§žê²Œ ì¡°ì • (ì˜ˆ: youtube_comments)
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ë¶„ì„ ê²°ê³¼ë¥¼ ìœ„í•´ ë¦¬í„´í•˜ê±°ë‚˜ íŠ¹ì • í…Œì´ë¸”ì— upsert
        try:
            # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ upsert (comment_id ê¸°ì¤€)
            data = self.supabase.table("youtube_comments").upsert(comments).execute()
            print(f"âœ… Saved {len(comments)} comments to Supabase.")
        except Exception as e:
            print(f"âŒ Supabase Save Error: {e}")

if __name__ == "__main__":
    scraper = YouTubeScraper()
    
    # ë¶„ì„ ì‹œê¸° ì„¤ì •
    periods = [
        {"name": "ì„±ìˆ™ê¸°(í˜„ìž¬)", "after": "2026-01-01T00:00:00Z", "before": "2026-01-14T23:59:59Z"},
        {"name": "ë„ìž…ê¸°(ê³¼ê±°)", "after": "2025-10-01T00:00:00Z", "before": "2025-11-30T23:59:59Z"}
    ]
    
    keyword = "ë‘ë°”ì´ ì«€ë“ ì¿ í‚¤"
    
    for period in periods:
        print(f"\nðŸ“º Analyzing Period: {period['name']}")
        videos = scraper.search_videos(keyword, published_after=period["after"], published_before=period["before"], max_results=5)
        
        for v in videos:
            print(f"  ðŸ” Video: {v['title']} ({v['published_at']})")
            comments = scraper.get_video_comments(v["video_id"], max_results=50)
            # scraper.save_comments_to_supabase(comments) # í…Œì´ë¸” ìƒì„± í›„ ì£¼ì„ í•´ì œ
            print(f"     -> Collected {len(comments)} comments")
