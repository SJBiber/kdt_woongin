import requests
import os
import json
import re
from datetime import datetime
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from supabase import create_client, Client

# .env íŒŒì¼ ë¡œë“œ
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, "../../.env")

if os.path.exists(env_path):
    load_dotenv(env_path)

class NaverBlogScraper:
    def __init__(self, client_id=None, client_secret=None):
        self.client_id = client_id or os.getenv("NAVER_CLIENT_ID")
        self.client_secret = client_secret or os.getenv("NAVER_CLIENT_SECRET")
        self.gemini_key = os.getenv("GEMINI_API_KEY")
        
        # Supabase ì„¤ì •
        self.supabase_url = os.getenv("SUPABASE_URL")
        self.supabase_key = os.getenv("SUPABASE_KEY")
        
        self.base_url = "https://openapi.naver.com/v1/search/blog.json"

        if not self.client_id or not self.client_secret:
            raise ValueError("Naver API Credentials are missing in .env file.")
        
        if not self.supabase_url or not self.supabase_key:
            raise ValueError("Supabase Credentials are missing in .env file.")

        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)

    def search_blog(self, query, display=100, start=1, sort='sim'):
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        # sort='sim'ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì •í™•ë„ìˆœ(ìœ ì‚¬ë„ìˆœ) ìˆ˜ì§‘
        params = {"query": query, "display": display, "start": start, "sort": sort}
        try:
            response = requests.get(self.base_url, headers=headers, params=params)
            return response.json() if response.status_code == 200 else None
        except Exception as e:
            print(f"API Request Error: {e}")
            return None

    def clean_text(self, text):
        if not text: return ""
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'[^\w\sê°€-í£.,!?%]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def get_blog_content(self, url):
        try:
            if "blog.naver.com" in url and "/PostView.naver" not in url:
                parts = url.split("/")
                if len(parts) >= 5:
                    user_id, log_no = parts[3], parts[4]
                    url = f"https://blog.naver.com/PostView.naver?blogId={user_id}&logNo={log_no}"

            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            
            content_div = soup.find("div", class_="se-main-container") or soup.find("div", id="postViewArea")
            return content_div.get_text(separator=" ").strip() if content_div else ""
        except Exception:
            return ""

    def save_to_supabase(self, data):
        try:
            # raw_contentëŠ” ì €ì¥í•˜ì§€ ì•Šë„ë¡ ë³€ê²½
            response = self.supabase.table("blog_review").upsert({
                "title": data['title'],
                "link": data['link'],
                "postdate": data['postdate'],
                "address": data['address'],
                "clean_content": data['clean_content']
            }).execute()
            return response
        except Exception as e:
            # ì¤‘ë³µ í‚¤ ì—ëŸ¬(23505) ë˜ëŠ” ì¤‘ë³µ ë¬¸êµ¬ í¬í•¨ ì‹œ ë¬´ì‹œ
            error_str = str(e).lower()
            if '23505' in error_str or 'duplicate key' in error_str:
                return None
            print(f"Supabase Save Error: {e}")
            return None

if __name__ == "__main__":
    scraper = NaverBlogScraper()
    
    # ì„œìš¸ì‹œ 25ê°œ ìì¹˜êµ¬ ë¦¬ìŠ¤íŠ¸
    seoul_districts = [
        "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬",
        "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬",
        "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
    ]
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ë‹¤ì–‘í™” (ì¤‘ë³µ íšŒí”¼ìš©)
    search_keywords = ["ë‘ë°”ì´ ì«€ë“ ì¿ í‚¤ ë§›ì§‘", "ë‘ë°”ì´ ì´ˆì½œë¦¿ ì¿ í‚¤", "ë‘ë°”ì´ ì¿ í‚¤ íŒŒëŠ”ê³³", "ë‘ë°”ì´ ì«€ë“ì¿ í‚¤ í›„ê¸°"]
    
    total_collected = 0
    target_total = 300 # 1000ê°œì—ì„œ 300ê°œë¡œ í•˜í–¥ ì¡°ì •
    
    print(f"ğŸš€ Starting Diverse Small Scale Collection (Target: {target_total} Seoul items)...")

    for keyword in search_keywords:
        if total_collected >= target_total: break
        
        for district in seoul_districts:
            if total_collected >= target_total: break
            
            query = f"ì„œìš¸ {district} {keyword}"
            print(f"\nğŸ” Searching: [{query}] (Unique Found: {total_collected})")
            
            # 300ê°œ ëª©í‘œì— ë§ì¶° ì§€ì—­ë³„ ìˆ˜ì§‘ ê°œìˆ˜ë¥¼ 20ê°œë¡œ ì œí•œí•˜ì—¬ ê³¨ê³ ë£¨ ìˆ˜ì§‘
            search_result = scraper.search_blog(query, display=20, start=1)
            
            if not search_result or 'items' not in search_result:
                continue
                
            items = search_result.get("items", [])
            for idx, item in enumerate(items):
                # ì œëª©ì—ì„œ HTML íƒœê·¸ ì œê±° (<b> ë“±)
                clean_title = scraper.clean_text(item['title'])
                
                raw_content = scraper.get_blog_content(item['link'])
                if not raw_content: continue
                
                clean_content = scraper.clean_text(raw_content)
                
                # 2. ì£¼ì†Œ ë° ìƒí˜¸ëª… ì¶”ì¶œ ì‹œë„
                addr_pattern = rf"(ì„œìš¸íŠ¹ë³„ì‹œ|ì„œìš¸ì‹œ)\s+([ê°€-í£]*{district}[ê°€-í£]*)\s+([ê°€-í£\d\s-]+(ë¡œ|ê¸¸|ë™|ê°€|ë²ˆì§€))"
                match = re.search(addr_pattern, clean_content)
                
                # íƒ€ ì§€ì—­ í‚¤ì›Œë“œ ì²´í¬ (ë§¤ìš° ì—„ê²©í•˜ê²Œ)
                other_regions = ["ì œì£¼", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ìˆ˜ì›", "ì„±ë‚¨", "ê³ ì–‘", "ìš©ì¸", "ì²œì•ˆ", "ì²­ì£¼"]
                # ë³¸ë¬¸ ì‹œì‘ í˜¹ì€ íŠ¹ì • í‚¤ì›Œë“œ ì£¼ë³€ì— íƒ€ ì§€ì—­ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
                if any(region in clean_content[:150] for region in other_regions) and district not in clean_content[:100]:
                    continue

                if match:
                    address = match.group(0)
                else:
                    # ìƒì„¸ ì£¼ì†Œê°€ ì—†ë”ë¼ë„ ì„œìš¸ ë°ì´í„°ì„ì´ í™•ì‹¤ì¹˜ ì•Šìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì„ íƒ)
                    # ì—¬ê¸°ì„œëŠ” ìˆ˜ì§‘ íš¨ìœ¨ì„ ìœ„í•´ ìƒì„¸ ì£¼ì†Œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µí•©ë‹ˆë‹¤.
                    continue

                db_data = {
                    'title': clean_title, 
                    'link': item['link'], 
                    'postdate': item['postdate'],
                    'address': address, 
                    'clean_content': clean_content
                }
                
                if scraper.save_to_supabase(db_data):
                    total_collected += 1
                    if total_collected % 10 == 0:
                        print(f"âœ… Unique Collected: {total_collected}...")
                
                if total_collected >= target_total:
                    break

    print(f"\nâœ¨ Mission Accomplished! Total {total_collected} Seoul data synced to Supabase.")
