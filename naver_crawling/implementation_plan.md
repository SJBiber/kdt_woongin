# 개발 착수서: 네이버 블로그 검색 기반 일별 포스팅 수 수집 프로그램

본 문서는 네이버 검색 API를 활용하여 특정 키워드에 대한 최근 180일간의 블로그 포스팅 수를 집계하고 Supabase에 저장하는 프로그램의 개발 계획을 담고 있습니다.

## 1. 프로젝트 개요
- **목적**: 특정 키워드의 최근 180일(어제 기준) 일별 블로그 업로드 개수 파악
- **핵심 전략**: API 호출 최소화를 위해 전체 데이터를 가져와 날짜별로 그룹화하여 집계
- **기술 스택**: Python, Naver Search API, Supabase

## 2. 주요 작업 단계

### Phase 1: 환경 설정 및 기초 설계
- [ ] `.env` 파일 구성 (NAVER_CLIENT_ID, NAVER_CLIENT_SECRET, SUPABASE_URL, SUPABASE_KEY)
- [ ] 필요한 라이브러리 설치 (`requests`, `python-dotenv`, `supabase`)
- [ ] Supabase 테이블 스키마 설계 및 생성
    - 테이블명: `naver_blog_counts`
    - 컬럼: `id` (PK), `keyword` (TEXT), `target_date` (DATE), `post_count` (INTEGER), `created_at` (TIMESTAMP)

### Phase 2: 네이버 API 클라이언트 구현
- [ ] 네이버 블로그 검색 API 연동 함수 작성
- [ ] **페이징 처리(Pagination) 로직 구현**
    - `display`를 최대값(100)으로 설정
    - `start` 파라미터를 1부터 1000까지 증가시키며 반복 호출
    - 검색 결과의 `total` 값을 확인하여 루프 종료 조건 설정
- [ ] 검색 결과에서 업로드 날짜(`postdate`) 추출 및 정제 로직 구현

### Phase 3: 데이터 수집 및 처리 로직
- [ ] 최근 180일 기간 설정 및 필터링 로직
- [ ] **전체 데이터 집계(Aggregation)**
    - 페이징을 통해 수집한 모든 블로그 포스팅의 날짜 정보를 기반으로 일별 데이터 그룹화
    - 만약 특정 키워드의 검색 결과가 1,000개를 초과할 경우, API 제약사항(최대 1000개 수집 가능)에 따른 정합성 확보 방안 검토 (예: 필요한 경우 날짜 범위를 좁혀서 검색)
- [ ] 특정 키워드에 대해 수집된 데이터의 정합성 확인

### Phase 4: Supabase 데이터 연동
- [ ] 집계된 데이터를 Supabase 테이블에 Upsert(Insert or Update) 하는 로직 구현
- [ ] 데이터 중복 저장 방지 처리

### Phase 5: 최종 통합 및 테스트
- [ ] 전체 프로세스 자동화 스크립트 작성
- [ ] 180일치 데이터 수집 및 DB 저장 확인
- [ ] 에러 핸들링 및 로그 기록 추가

## 3. 고려 사항 및 제약 조건
- **API 제한**: 네이버 검색 API는 한 검색어당 최대 1,000개의 검색 결과만 제공합니다. 대량의 데이터가 발생하는 키워드의 경우 정확한 수치 산정에 한계가 있을 수 있습니다.
- **날짜 범위**: `금일 - 180일`부터 `어제`까지의 범위를 정확히 계산합니다.
- **확장성**: 향후 DAG(Airflow 등)를 통한 데일리 수집이 가능하도록 코드를 모듈화합니다.
