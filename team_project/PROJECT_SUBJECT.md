## 두바이 쫀득 쿠기의 시장성과 수익성을 분석하기 위한 데이터 기반 비즈니스 인사이트 도출

### 서울시 '두쫀쿠' 열풍의 경제적 실효성 및 수익성 분석


주제 : 데이터 크롤링을 통한 소비자 반응 수집 및 원자재 대비 마진율 산출

### 배경 및 목적

최근 SNS(TikTok, Instagram)를 중심으로 전 세계적 열풍을 일으킨 '두바이 초콜릿'과 그 한국형 변주인 **'두바이 쫀득 쿠키(두쫀쿠)'**는 디저트 시장의 새로운 강자로 떠오름.
하지만 이러한 인기의 이면에는 자영업자들이 직면한 심각한 비즈니스적 난제들이 존재.

1.  **원자재 리스크 (Raw Material Crisis)**: 
    -   피스타치오 도매가: 2024년 1kg당 4.5만 원 → 2026년 **10만 원 이상으로 약 122% 급등**.
    -   카다이프 수입량: 2023년 24톤 → 2024년 **304톤으로 약 1,166% 폭증**.
2.  **트렌드 휘발성 (Trend Fading)**: 단순 'Fad(반짝 유행)'인지 'Steady-seller(스테디셀러)'인지에 대한 데이터 기반 판단 근거 부족.
3.  **가격 책정 혼선**: 폭등한 원가를 반영하면서도 소비자가 납득할 수 있는 적정 가격에 대한 시장 반응성 검증 필요.

본 프로젝트는 이러한 문제를 해결하기 위해 네이버 블로그 리뷰, 유튜브 댓글 및 원자재 시세 데이터를 수집/분석하여, 
예비 창업자와 자영업자들을 위한 **데이터 기반 비즈니스 의사결정 가이드라인**을 제공하는 것을 목적.

### 분석 질문

- Q1. 사장 수요의 지속성 : 두쫀쿠 언급량 및 유튜브 참여도의 추이가 상승기인가? 아니면 '탕후루'와 같은 단기 유행의 말기인가?
- Q2. 실질 수익성 분석 : 폭등한 원자재가 대비 현재의 실제 판매가 데이터가 지속 가능한 이윤을 창출하는가?
- Q3. 유튜브 관심도 및 여론 변화 : 트렌드 형성기(25년 10/11월)와 현재(26년 1월)의 유튜브 댓글 반응 비교를 통해 트렌드 하락 신호가 포착되는가?

### 데이터 수집 계획

- Naver Search APT 이용(블로그)
    - 수집 대상 : '서울시 두바이 쫀득 쿠키' , '두쫀쿠 맛집', '두쫀쿠 매장 태그 게시물'
    - 수집 항목 : 포스팅 제목 , 본문 요약 , 게시 날짜 , 위치 태그(서울시)
    - 활용 : 본문 요약 분석을 통해 "가격 대비 만족도"와 "재구매 의사" 텍스트 마이닝

- 인스타그램 크롤링(Playwright)
    - 수집 대상 : '서울시 두바이 쫀득 쿠키' , '두쫀쿠 맛집', '두쫀쿠 매장 태그 게시물'
    - 수집 항목 :  좋아요 수, 댓글 내용 , 게시 날짜 , 위치 태그(서울시)
    - 활용 : 트랜드 분석 및 인플루언서 영향력이 실제 품절 대란에 미치는 상관관계### 과거 유행 사례의 교훈 (Fad vs Trend)

## 2. 문제 정의 (Why)

### 해결하고자 하는 문제
"감(Intuition)에 의존한 메뉴 도입과 원가 부담으로 인한 소상공인의 폐업 리스크 및 수익 구조 불투명성"

### 분석 수식

 1. 상인들의 이윤 계산을 위한 가상 변수 설정 및 수식 적용
    
    - 실제 수익 = 총 판매수익 - (원자재비 + 운영비)
    
    - 원자재비 : 카다이프면 , 피스타치오 페이스트 , 마시넬로 , 쿠키 반죽의 시세 반영

    - 이윤율 분석 : 수집된 평균 판매가(S)와 추정 원가(C)를 비교해서 마진율 계산

    - 마진율 = (S-C)/S * 100

 2. 유튜브 댓글 시계열 관심도 분석
    - 특정 시점(도입기 vs 성숙기) 게시물의 신규 댓글 유입률(Decay Rate) 측정
    - 시간 흐름에 따른 감성 키워드(신기함 -> 가격 사악/질림) 전이 양상 분석
    - 과거 인기 영상에 대한 현재의 무관심도(No comments)를 통한 유행 종료 시점 예측


### 계획

-----------------
단계 | 주요업무 | 산출물
|---|---|---|
| phase1|API 연동 및 크롤러 환경 구축 | 데이터 수집 파이프라인
| phase2|Raw 데이터 전처리 및 분석 | 정제된 DB 데이터 , 키워드 데이터
| phase3|원가 시뮬레이션 및 수익성 모델링 | 마진율 분석 보고서
| phase4|최종 인사이트 도출 및 시각화 | 비즈니스 대시보드


### 기대 효과

- 예비 창업자 : 원자재 리스크와 적정 판매가에 대한 데이터 가이드라인 제공
- 현식 상인 : 소셜 데이터를 활용한 재고 관리 최적화 및 타겟 마케팅 지역 선정


### Phase1 크롤링 프로그램 진행 상황 및 시행착오 (AI와 함께 진행)
#### Phase 1 체크리스트
- [x] Naver API 호출 성공 및 데이터 반환 확인
- [ ] 인스타그램 로그인 및 동적 데이터 로딩 성공 여부
- [x] 수집된 데이터의 인코딩 문제 해결 (한글 깨짐 방지)
- [ ] 데이터 수집 주기 설정 (Daily Batch 준비)

#### Phase 1 시행착오
1. 네이버 크롤링까지는 진행함
    - 크롤링시 동일한 블로그 주소에 들어가서 데이터 수집 (같은 리뷰 데이터가 2번 저장하는 버그 확인)
        - 예외처리를 하기위해 테이블 블로그 url에 유니크 키를 설정해서 중복으로 db에 들어가지 않도록함
        - 파이썬 로직에도 response = self.supabase.table("blog_review").upsert()로 response를 세팅 후 예외처리 될 수 있도록 함
2. 크롤링 후 리뷰 데이터를 ai로 돌려보면 정규표현식 사용하지 않고 분석할수있겠다! 라고 생각하여 gemini_analyzer를 구현 
    - gemini api를 사용하여 리뷰 데이터를 분석하도록 프로그램 구성
        - 이슈 1. 6번째때 에러발생 -> 무료 티어 제한 (RPM 5)으로 sleep 시간을 더 줌
        - 이슈 2. 무료 api는 하루 쿼타가 20번으로 제한되어 있어 일일 20개의 리뷰만 분석가능
        - 이슈 3. 너무 느림
        - 이슈 회고 : 정규표현식을 사용하면 불필요한 자원 손실을 줄일것이라고 생각했지만 , AI한테 맡기면 편할거라는 생각으로 인해 진행했지만 수정 필요함 

3. 크롤링 분석을 ai에게 맡기는 것보다는 리뷰 데이터를 불러와서 정규 표현식을 사용하여 원하는 단어 , 기대 단어별로 분석하여 추출하는 방식으로 함 ( 리뷰 항목의 데이터 (비쌈 , 저렴 , 맛있음 , 맛없음 , 방문의사 등)을 하드코딩하여 분석 진행 )

