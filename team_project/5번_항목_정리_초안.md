# 5. 트러블 슈팅 (Problem Solving)

## 5.1 유튜브 API 호출 횟수 초과 및 데이터 누락 문제

### [문제 상황]
- 특정 기간(3개월) 동안의 동영상 데이터를 수집하던 중, 유튜브 API의 일일 할당량(Quota)을 급격하게 소모하여 크롤링이 중단되는 현상 발생.
- **원인 분석**: 
  1. 일 단위(`180일`)로 API를 반복 호출하면서 기본 호출 횟수 증가.
  2. 수집된 영상 리스트의 각 항목에 대해 상세 정보를 다시 호출하는 구조로 인해 API 호출량이 기하급수적으로 늘어남.

### [해결 과정]
- **시도 1 (실패)**: 일별 호출 대신 30일 단위로 기간을 넓혀 API 호출 횟수를 줄이려 함.
  - **부작용**: 유튜브 `search.list` API의 페이지네이션 제한(최대 약 500개)으로 인해, 업로드량이 많은 12월 데이터가 대거 누락되는 현상 발견.
- **최종 해결책**: 영상 업로드 날짜 필터(`publishedAfter`, `publishedBefore`)의 간격을 **5일 단위**로 세분화하여 호출.
  - 호출 횟수를 획기적으로 줄이면서도(`180회` -> `36회`), 500개 제한 범위 내에서 데이터 누락 없이 모든 영상을 안정적으로 수집함.

---

## 5.2 데이터베이스(DB) 내 중복 데이터 발생 이슈

### [문제 상황]
- 수집 프로그램을 재실행할 때마다 동일한 날짜의 데이터가 기존 테이블에 중복으로 `INSERT`되어, 통계 데이터의 정확도가 떨어지는 현상 발생.

### [해결 과정]
- **대안 1**: 데이터 삽입 전 미리 `SELECT` 쿼리로 날짜 존재 여부를 확인하고 `UPDATE` 또는 `INSERT` 결정.
  - **단점**: 매번 조회를 수행하므로 DB 자원 낭비 및 처리 속도 저하.
- **최종 해결책**: SQL의 **`ON CONFLICT`** 구문을 활용한 **UPSERT(Update + Insert)** 로직 적용.
  - `날짜`와 `키워드`를 고유 제약 조건(Unique Constraint)으로 설정하고, 충돌 발생 시 기존 데이터를 새로운 값으로 `UPDATE`하도록 로직을 수정하여 성능과 정합성을 모두 확보함.

## 5.3 Airflow Dag 동작시 db connection Parameter 문제

### [문제 상황]
- Airflow Dag를 실행할 때마다 DB 데이터가 저장되지않는 이슈

### [해결 과정]
- **원인**: Airflow Dag 동작시 개인이 사용하는 db connection 값을 바라보고 있어 프로젝트 데이터베이스가 아닌 개인 데이터베이스 테이블에 생성되는것 확인
- **최종 해결책**: 공유 프로젝트 db connection 값을 생성 및 사용하여 이슈 해결.


## 5.4 Airflow Task Runner 내 sys.stdout 재정의로 인한 I/O 오류

### [문제 상황]
- Airflow DAG를 통해 파이썬 크롤링 스크립트 실행 시, `ValueError: I/O operation on closed file` 오류가 발생하며 작업이 중단되는 현상.

### [해결 과정]
- **원인 분석**: 
    - 윈도우 로컬 환경에서의 한글 출력 깨짐을 방지하기 위해 코드 상단에 `sys.stdout`을 리디렉션하거나 인코딩을 강제로 변경하는 코드가 포함되어 있었음.
    - Airflow의 **Task Runner**는 작업 로그를 캡처하고 관리하기 위해 자체적으로 표준 출력(`stdout`)을 제어하는데, 스크립트 내부에서 이를 강제로 재정의하거나 닫으려 시도하면서 파일 핸들 충돌 및 닫힌 파일에 대한 I/O 작업 오류가 발생함.
- **최종 해결책**: 
    - 문제가 된 `sys.stdout` 리디렉션 코드를 삭제하고, 불필요해진 `sys`, `io` 임포트를 제거함.
    - Airflow 환경에서는 시스템이 제공하는 로깅 핸들러가 한글 출력을 적절히 처리하므로, 개별 스크립트에서의 강제적인 `stdout` 제어 없이도 안정적으로 동작함을 확인.

---
• **배운 점**:
1. **AI 협업을 통한 효율적 개발**: `AGENT.md` 파일에 명확한 개발 사양을 정의함으로써, AI와 협업하여 크롤링 로직을 신속하고 체계적으로 구현하는 프로세스를 경험했습니다.
2. **워크플로우 자동화 이해**: Airflow DAG의 기본 구조와 동작 원리를 학습하며, 데이터 수집 파이프라인 자동화 및 관리의 중요성을 체감했습니다.
3. **환경별 특성에 따른 개발 역량 강화**: 로컬(Windows) 환경과 실제 배포(Airflow) 환경의 실행 컨텍스트 차이를 명확히 이해하게 되었습니다. 특히 시스템 자원(stdout 등) 제어 시 발생할 수 있는 환경 충돌 이슈를 해결하며 더욱 견고한 코드를 작성하는 법을 배웠습니다.