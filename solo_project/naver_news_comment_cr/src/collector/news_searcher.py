"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° URL ìˆ˜ì§‘ ëª¨ë“ˆ
ì •ì  í¬ë¡¤ë§ ìš°ì„ , í•„ìš”ì‹œ ë„¤ì´ë²„ ê²€ìƒ‰ API ì‚¬ìš©
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import quote
import time
from config.settings import NAVER_CLIENT_ID, NAVER_CLIENT_SECRET, MAX_NEWS_COUNT


class NaverNewsSearcher:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° URL ìˆ˜ì§‘ í´ëž˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        self.use_api = bool(NAVER_CLIENT_ID and NAVER_CLIENT_SECRET)
        
        if self.use_api:
            print("âœ… ë„¤ì´ë²„ ê²€ìƒ‰ API ì‚¬ìš©")
        else:
            print("âœ… ì›¹ í¬ë¡¤ë§ ë°©ì‹ ì‚¬ìš© (API ë¯¸ì„¤ì •)")
    
    def search_news_by_api(self, keyword: str, max_count: int = MAX_NEWS_COUNT) -> List[Dict]:
        """
        ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•œ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            ë‰´ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'title', 'link', 'description', 'pubDate'}]
        """
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            'X-Naver-Client-Id': NAVER_CLIENT_ID,
            'X-Naver-Client-Secret': NAVER_CLIENT_SECRET
        }
        params = {
            'query': keyword,
            'display': min(max_count, 100),  # API ìµœëŒ€ 100ê°œ
            'sort': 'date'  # ìµœì‹ ìˆœ
        }
        
        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            
            news_list = []
            for item in data.get('items', []):
                # ë„¤ì´ë²„ ë‰´ìŠ¤ë§Œ í•„í„°ë§ (ëŒ“ê¸€ì´ ìžˆëŠ” ë‰´ìŠ¤)
                if 'news.naver.com' in item['link']:
                    news_list.append({
                        'title': self._clean_html_tags(item['title']),
                        'link': item['link'],
                        'description': self._clean_html_tags(item['description']),
                        'pubDate': item['pubDate']
                    })
            
            print(f"âœ… APIë¡œ {len(news_list)}ê°œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
            
        except Exception as e:
            print(f"âŒ API ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_news_by_crawling(self, keyword: str, max_count: int = MAX_NEWS_COUNT) -> List[Dict]:
        """
        ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            ë‰´ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        encoded_keyword = quote(keyword)
        news_list = []
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ í¬ë¡¤ë§
        for start in range(1, max_count + 1, 10):
            url = f"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&start={start}"
            
            try:
                response = requests.get(url, headers=self.headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‰´ìŠ¤ í•­ëª© ì°¾ê¸°
                news_items = soup.select('div.news_area')
                
                for item in news_items:
                    if len(news_list) >= max_count:
                        break
                    
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_elem = item.select_one('a.news_tit')
                    if not title_elem:
                        continue
                    
                    link = title_elem.get('href', '')
                    
                    # ë„¤ì´ë²„ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                    if 'news.naver.com' not in link:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    
                    # ìš”ì•½ ì¶”ì¶œ
                    desc_elem = item.select_one('div.news_dsc')
                    description = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    news_list.append({
                        'title': title,
                        'link': link,
                        'description': description,
                        'pubDate': ''  # í¬ë¡¤ë§ì—ì„œëŠ” ë‚ ì§œ ì •ë³´ ì œí•œì 
                    })
                
                if len(news_list) >= max_count:
                    break
                
                time.sleep(0.5)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                
            except Exception as e:
                print(f"âŒ íŽ˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ (start={start}): {e}")
                continue
        
        print(f"âœ… í¬ë¡¤ë§ìœ¼ë¡œ {len(news_list)}ê°œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")
        return news_list
    
    def search_news(self, keyword: str, max_count: int = MAX_NEWS_COUNT) -> List[Dict]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ (API ìš°ì„ , ì‹¤íŒ¨ì‹œ í¬ë¡¤ë§)
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            ë‰´ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ðŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        if self.use_api:
            news_list = self.search_news_by_api(keyword, max_count)
            if news_list:
                return news_list
            print("âš ï¸  API ê²€ìƒ‰ ì‹¤íŒ¨, í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜")
        
        return self.search_news_by_crawling(keyword, max_count)
    
    @staticmethod
    def _clean_html_tags(text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return BeautifulSoup(text, 'html.parser').get_text()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    searcher = NaverNewsSearcher()
    results = searcher.search_news("AI", max_count=5)
    
    for i, news in enumerate(results, 1):
        print(f"\n{i}. {news['title']}")
        print(f"   URL: {news['link']}")
