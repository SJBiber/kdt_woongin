"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
í‚¤ì›Œë“œ ì…ë ¥ â†’ ë‰´ìŠ¤ ê²€ìƒ‰ â†’ ëŒ“ê¸€ í¬ë¡¤ë§ â†’ DB ì €ì¥
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from src.collector.news_searcher import NaverNewsSearcher
from src.collector.comment_crawler import NaverCommentCrawler
from database.supabase_manager import SupabaseManager
from config.settings import validate_config, MAX_NEWS_COUNT, MAX_COMMENTS_PER_NEWS
import time


def collect_and_save_comments(keyword: str, max_news: int = MAX_NEWS_COUNT, 
                               max_comments: int = MAX_COMMENTS_PER_NEWS):
    """
    í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ ëŒ“ê¸€ ìˆ˜ì§‘ ë° DB ì €ì¥
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_news: ìµœëŒ€ ë‰´ìŠ¤ ìˆ˜
        max_comments: ë‰´ìŠ¤ë‹¹ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜
    """
    print("=" * 60)
    print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“Œ ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
    print(f"ğŸ“Œ ìˆ˜ì§‘ ë‰´ìŠ¤ ìˆ˜: ìµœëŒ€ {max_news}ê°œ")
    print(f"ğŸ“Œ ë‰´ìŠ¤ë‹¹ ëŒ“ê¸€ ìˆ˜: ìµœëŒ€ {max_comments}ê°œ")
    print("=" * 60)
    
    # 1. í™˜ê²½ ì„¤ì • ê²€ì¦
    try:
        validate_config()
    except ValueError as e:
        print(f"\nâŒ {e}")
        return
    
    # 2. ë‰´ìŠ¤ ê²€ìƒ‰
    searcher = NaverNewsSearcher()
    news_list = searcher.search_news(keyword, max_news)
    
    if not news_list:
        print("\nâŒ ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
    
    # 3. ëŒ“ê¸€ í¬ë¡¤ë§ ë° DB ì €ì¥
    crawler = NaverCommentCrawler(headless=True)
    db = SupabaseManager()
    
    total_comments = 0
    total_saved = 0
    
    for idx, news in enumerate(news_list, 1):
        print(f"\n{'=' * 60}")
        print(f"[{idx}/{len(news_list)}] {news['title'][:50]}...")
        print(f"URL: {news['link']}")
        print(f"{'=' * 60}")
        
        # ëŒ“ê¸€ í¬ë¡¤ë§
        comments = crawler.crawl_comments(news['link'], max_comments)
        total_comments += len(comments)
        
        if not comments:
            print("âš ï¸  ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ìˆ˜ì§‘ ì‹¤íŒ¨")
            continue
        
        # ì¤‘ë³µ ì²´í¬ ë° í•„í„°ë§
        new_comments = []
        for comment in comments:
            if not db.comment_exists(comment['comment_id']):
                new_comments.append(comment)
        
        if not new_comments:
            print(f"âš ï¸  ëª¨ë“  ëŒ“ê¸€ì´ ì´ë¯¸ DBì— ì¡´ì¬í•¨ (ì¤‘ë³µ {len(comments)}ê°œ)")
            continue
        
        print(f"ğŸ’¾ ìƒˆë¡œìš´ ëŒ“ê¸€ {len(new_comments)}ê°œ ì €ì¥ ì¤‘...")
        
        # DB ì €ì¥
        saved_count = db.insert_comments_batch(new_comments)
        total_saved += saved_count
        
        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ê³¼ë„í•œ í¬ë¡¤ë§ ë°©ì§€)
        if idx < len(news_list):
            print("â³ ë‹¤ìŒ ë‰´ìŠ¤ ì²˜ë¦¬ê¹Œì§€ 3ì´ˆ ëŒ€ê¸°...")
            time.sleep(3)
    
    # 4. í¬ë¡¤ëŸ¬ ì¢…ë£Œ
    crawler.close()
    
    # 5. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("âœ… ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“Š ì²˜ë¦¬í•œ ë‰´ìŠ¤: {len(news_list)}ê°œ")
    print(f"ğŸ“Š ìˆ˜ì§‘í•œ ëŒ“ê¸€: {total_comments}ê°œ")
    print(f"ğŸ“Š ì €ì¥í•œ ëŒ“ê¸€: {total_saved}ê°œ")
    print(f"ğŸ“Š ì¤‘ë³µ ì œì™¸: {total_comments - total_saved}ê°œ")
    print("=" * 60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘ê¸°")
    print("-" * 60)
    
    # í‚¤ì›Œë“œ ì…ë ¥
    keyword = input("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if not keyword:
        print("âŒ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    # ìˆ˜ì§‘ ì‹œì‘
    collect_and_save_comments(keyword)


if __name__ == "__main__":
    main()
